---
title: 'Statistical Physics and Applications'
date: 2022-11-28
permalink: /posts/2022/11/28/blog-post/
tags:
  - Statistical Physics
---

# Purpose

I write this note to help myself learn and understand statistical physics. Also for using English in academic.

# Maximum Entropy Principle

The begin of statistical physics is *Maximum Entropy Principle*. So what is *Maximum Entropy Principle*, or more basicly, what is *Entropy*?

The object $O$ in *Maximum Entropy Principle* can have many *states* $s_i$ and what we care about is the possibility on these *states* $p_i$. I write this opinion down

$$
\begin{align}
&O \sim s_1,s_2,\cdots,s_N \notag \\
&S = S(p_1,p_2,\cdots,p_N) \notag
\end{align}
$$

So *Maximum Entropy Principle* will give a constraint *F* on the *states* $p_i$. But what is the explicit form of the constraint *F* under specific conditions? We must consider the concrete meaning of *states* ${s_i}$ and the explicit form of *Entropy* $S$ on these *states*.


### Uniform Distribution and Shannon Entropy

If the object $O$ has $N$ *states* totally. Now we can check the probability distribution is uniform distribution by *Maximum Entropy Principle* with Shannon Entropy $S = \sum_i-p_ilnp_i$. We use Lagrange Multiplier
$$
S = -\sum_i p_ilnp_i - \alpha(\sum_i p_i-1)
$$
When the probability $p_i$ has maximum entropy, we have
$$
\frac{\partial S}{\partial p_i} = -(lnp_i+p_i*\frac{1}{p_i}) - \alpha=0\\
lnp_i = -\alpha-1\\
p_i = e^{-\alpha-1}\\
\sum_i p_i = \sum_i e^{-\alpha-1} = N*e^{-\alpha-1} = 1\\
p_i = 1/N.
$$

Similarly, we can consider many other constraint. If every *state* $s_i$ has an energy $E_i$ and the average energy is constrained to a fixed value $E$, then
$$
S = -\sum_i p_ilnp_i - \alpha(\sum_i p_i-1) - \beta(\sum_i p_iE_i - E)\\
\frac{\partial S}{\partial p_i} = -(lnp_i+p_i*\frac{1}{p_i}) - \alpha-\beta E_i=0\\
lnp_i = -\alpha-\beta E_i-1\\
p_i = e^{-\alpha-\beta E_i-1}\\
\sum_i p_i-1 = \sum_i e^{-\alpha-\beta E_i-1} - 1 = 0\\
e^{-\alpha-1} = \frac{1}{\sum_i e^{-\beta E_i}} \\
p_i = \frac{e^{-\beta E_i}}{\sum_i e^{-\beta E_i}}.
$$
This is just Maxwell-Boltzmann distribution.

#### History

Firstly expounded by E. T. Jaynes(J in JC model).

## Thermal Entropy

## Microstates and Macrostates

The states can be microstates and also macrostates. When we say they are microstates, that means we can't or needn't find any internal variables or freedoms to divide one microstate into more states. So every microstate has equal weight, that means the possibility of each microstate is equal to $1/N$, where $N$ is the total number of microstates. So we have microstates but why do we need macrostates? Beacause many microstates can have same features and too many microstates is meaningless expect their huge total number $N$. Nothing else we can know. And what we care about are a few important physical variables describing the objects. So can we fetch the information about these physical variables? The answer is just *Maximum Entropy*, or more accurately, following the direction and method shown by *Maximum Entropy*.

$$S=k_blnW$$

Let me use Maximum Entropy get Maxwell-Boltzmann distribution, Bose-Einstein distribution and Fermi-Dirac distribution.

## Maxwell-Boltzmann Distribution

We consider the model *M* consist of the system *S* and a thermal bath *B*. The total energy $E_t$ and particle numbers $N_t$ in *M* are conserved. But the system *S* and bath *B* can exchange energy and particle with each other.

### Only exchange energy

Let me define the conditions more accurately with mathmatics

$$E_t = E_s + E_b = Const$$

$$W_t = W_s*W_b$$

(Assumption of independence between *S* and *B*, but if they have correlation?)
Now we want to maximize $W_t$ and need to point out the relation between the number of microscopic states $W$ and energy $E$.

