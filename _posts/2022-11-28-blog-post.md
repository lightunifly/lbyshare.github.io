---
title: 'Statistical Physics and Applications'
date: 2022-11-28
permalink: /posts/2022/11/28/blog-post/
tags:
  - Statistical Physics
---

# Purpose

I write this note to help myself learn and understand statistical physics. Also for using English in academic.

# Maximum Entropy Principle

Statistical Mechanics is the third pillar of modern physics, next to quantum theory and relativity theory. Statistical physics has not yet found a generally accepted theoretical framework or a canonical formalism. (Roman Frigg)

The begin of statistical physics is *Maximum Entropy Principle*. So what is *Maximum Entropy Principle*, or more basicly, what is *Entropy*?

The object $O$ in *Maximum Entropy Principle* can have many *states* $s_i$ and what we care about is the possibility on these *states* $p_i$. I write this opinion down

$$
\begin{align}
&O \sim s_1,s_2,\cdots,s_N \notag \\
&S = S(p_1,p_2,\cdots,p_N) \notag
\end{align}
$$

So *Maximum Entropy Principle* will give a constraint *F* on the *states* $p_i$. But what is the explicit form of the constraint *F* under specific conditions? We must consider the concrete meaning of *states* ${s_i}$ and the explicit form of *Entropy* $S$ on these *states*.


## Uniform Distribution and Shannon Entropy

If the object $O$ has $N$ *states* totally. Now we can check the probability distribution is uniform distribution by *Maximum Entropy Principle* with Shannon Entropy $S = \sum_i-p_ilnp_i$. We use Lagrange Multiplier
$$
S = -\sum_i p_ilnp_i - \alpha(\sum_i p_i-1)
$$
When the probability $p_i$ has maximum entropy, we have
$$
\frac{\partial S}{\partial p_i} = -(lnp_i+p_i*\frac{1}{p_i}) - \alpha=0\\
lnp_i = -\alpha-1\\
p_i = e^{-\alpha-1}\\
\sum_i p_i = \sum_i e^{-\alpha-1} = N*e^{-\alpha-1} = 1\\
p_i = 1/N.
$$

Similarly, we can consider many other constraint. If every *state* $s_i$ has an energy $E_i$ and the average energy is constrained to a fixed value $E$, then
$$
S = -\sum_i p_ilnp_i - \alpha(\sum_i p_i-1) - \beta(\sum_i p_iE_i - E)\\
\frac{\partial S}{\partial p_i} = -(lnp_i+p_i*\frac{1}{p_i}) - \alpha-\beta E_i=0\\
lnp_i = -\alpha-\beta E_i-1\\
p_i = e^{-\alpha-\beta E_i-1}\\
\sum_i p_i-1 = \sum_i e^{-\alpha-\beta E_i-1} - 1 = 0\\
e^{-\alpha-1} = \frac{1}{\sum_i e^{-\beta E_i}} \\
p_i = \frac{e^{-\beta E_i}}{\sum_i e^{-\beta E_i}}.
$$
This is just Maxwell-Boltzmann distribution.

### Thermal Entropy

We know the thermal entropy $S=k_blnW$, where $W$ is the number of microstates. Now we show that we can get thermal entropy from Shannon entropy(Gibbs entropy strictly).

$$
S = -\sum_{i=1}^{W} p_ilnp_i\\
S = -\sum_{i=1}^{W} \frac{1}{W}ln\frac{1}{W}\\
S = lnW
$$

### Von-Neumann Entropy

When we walk into quantum mechanics, we should consider von-Neumann entropy $S = -{\rm Tr}(\rho ln\rho)$. Let we show von-Neumann entropy is an extension of Shannon entropy(Gibbs entropy)
$$
S = -{\rm Tr}(\rho ln\rho)\\
if \rho = \sum_i p_i|i\rangle\langle i|\\
S = -{\rm Tr}[(\sum_i p_i|i\rangle\langle i|)ln(\sum_i p_i|i\rangle\langle i|)]\\
S = -{\rm Tr}[(\sum_i p_i|i\rangle\langle i|)(\sum_i lnp_i|i\rangle\langle i|)]\\
S = -{\rm Tr}(\sum_i p_ilnp_i|i\rangle\langle i|)=-\sum_i p_ilnp_i
$$
Now let we see what we can get from *Maximum Entropy Principle* with von-Neumann entropy. The answer is Bose-Einstein distribution and Fermi-Dirac distribution. We must consider what is the meaning of maximum entropy for $\rho$ and what is the constrains in quantum mechanics?

+ We firstly define the system and constrains: The energy and the number of particles in the system is conserved.
$$
{\rm Tr}(\rho\hat{H})=E\\
{\rm Tr}(\rho\hat{N})=N\\
\left[\hat{H},\hat{N}\right]=0
$$
Beacause Hamiltonian and the particle number operator commute, there exists a basis in which both are diagonal. We choose the basis $|i\rangle$ to consider the problem in hand. Remember now Hamiltonian and the particle number operator are diagonal, but the density matrix $\rho$ is uncertain.

+ If we use Langrange multiplier as in classical mechanics, we can get

$$
S = -{\rm Tr}(\rho ln\rho) - \alpha({\rm Tr}(\rho\hat{H})-E) - \beta({\rm Tr}(\rho\hat{N})=N)\\
\hat{H},\hat{N}\text{ is diagonal, }S = -{\rm Tr}(\rho ln\rho) - \alpha(\sum_i\rho_{ii}H_{ii}-E) - \beta(\sum_i\rho_{ii}N_{ii}-N)
$$
so what is the partial derivative we should do now?
$$\frac{\partial S}{\partial\rho_{ij}}=0$$

+ "Among all density matrices with the same diagonal (in some basis), the matrix with all entries outside the diagonal equal to zeros has the largest von-Neumann entropy".
This is an important consequence of Schur's theorem, but what is Schur's theorem?

  + If $A$ is a $n*n$ complex square matrix, then there is an unitary matrix $Q$ and a $n*n$ upper triangular matrix $U$:
  $$
  A=QUQ^H
  $$
  + von-Neumann entropy is a Schur-concave function and preserves the majorization order on density matrices. So are these two equivalent?

    + $$if \rho_1 \succ \rho_2, S(\rho_1)\geq S(\rho_2).$$
    + $$\sum_i\lambda_iS(\rho_i)-\sum_i\lambda_i\ln\lambda_i\geq S(\sum_i\lambda_i\rho_i)\geq\sum_i\lambda_iS(\rho_i)$$

+ Now we can only need to consider off-diagonal elements in $\rho$.
$$
S = -\sum_i\rho_{ii}\ln\rho_{ii} - \alpha(\sum_i\rho_{ii}H_{ii}-E) - \beta(\sum_i\rho_{ii}N_{ii}-N)
$$
Now the problem is similar as in classical mechanics, we can get
$$
\rho = \frac{e^{-\beta(\hat{H}-\mu\hat{N})}}{\rm Tr(e^{-\beta(\hat{H}-\mu\hat{N})})}
$$
In order to get Bose-Einstein distribution and Fermi-Dirac distribution, we must assume the particles are non-interacting. That's meaning
$$
\hat{H}=\sum_iE_i\hat{N}_i
$$
so we can calculate the partition function<br>
$$
\begin{aligned}
Z &= \rm Tr(e^{-\beta(\hat{H}-\mu\hat{N})}) \\
&= \sum_{N=0}^{\infty}\sum_{N_i,\sum N_i=N}\prod_{i}e^{-\beta(E_iN_i-\mu N_i)} \\
&= \sum_{N=0}^{\infty}\sum_{N_i,\sum N_i=N}\prod_{i}e^{-\beta N_i(E_i-\mu)} \\
&= \prod_{i}\sum_{N=0}^{\infty}\sum_{N_i,\sum N_i=N}e^{-\beta N_i(E_i-\mu)}
\end{aligned}
$$
now we can see the difference between boson and fermion<br>
$$
\begin{aligned}
&\text{for boson}, \sum_{N=0}^{\infty}\sum_{N_i,\sum N_i=N}e^{-\beta N_i(E_i-\mu)}=\sum_{N_i=0}^{\infty}e^{-\beta N_i(E_i-\mu)}=\frac{1}{1-e^{-\beta(E_i-\mu)}}\\
&\text{for fermion}, \sum_{N=0}^{\infty}\sum_{N_i,\sum N_i=N}e^{-\beta N_i(E_i-\mu)}=1+e^{-\beta(E_i-\mu)}
\end{aligned}
$$
then we can get the partition in a unity form<br>
$$
Z = \prod_{i}(1+(-1)^se^{-\beta(E_i-\mu)})^{1-2s}.
$$
Then we use the partition function to calculate the number of particles in state $i$<br>
$$
\begin{aligned}
\rm Tr[\rho N_i] &= -\frac{1}{\beta}\frac{\partial}{\partial E_i}\ln Z\\
&=-\frac{1}{\beta}\frac{\partial}{\partial E_i}\sum_i\ln(1+(-1)^se^{-\beta(E_i-\mu)})^{1-2s}\\
&=-\frac{1}{\beta}(1-2s)\frac{(-1)^se^{-\beta(E_i-\mu)}\cdot(-\beta)}{1+(-1)^se^{-\beta(E_i-\mu)}}\\
&=(1-2s)\frac{(-1)^se^{-\beta(E_i-\mu)}}{1+(-1)^se^{-\beta(E_i-\mu)}}.
\end{aligned}
$$
And this is just<br>
$$
\begin{aligned}
&\text{for boson}, \rm Tr[\rho N_i] = -\frac{-e^{-\beta(E_i-\mu)}}{1-e^{-\beta(E_i-\mu)}}=\frac{1}{e^{\beta(E_i-\mu)}-1}\\
&\text{for fermion}, \rm Tr[\rho N_i] = \frac{e^{-\beta(E_i-\mu)}}{1+e^{-\beta(E_i-\mu)}}=\frac{1}{e^{\beta(E_i-\mu)}+1}.
\end{aligned}
$$

#### History

Firstly expounded by E. T. Jaynes(J in JC model).



#### Microstates and Macrostates

The states can be microstates and also macrostates. When we say they are microstates, that means we can't or needn't find any internal variables or freedoms to divide one microstate into more states. So every microstate has equal weight, that means the possibility of each microstate is equal to $1/N$, where $N$ is the total number of microstates. So we have microstates but why do we need macrostates? Beacause many microstates can have same features and too many microstates is meaningless expect their huge total number $N$. Nothing else we can know. And what we care about are a few important physical variables describing the objects. So can we fetch the information about these physical variables? The answer is just *Maximum Entropy*, or more accurately, following the direction and method shown by *Maximum Entropy*.


### Thinking and Discussion
+ Now we use Maximum Entropy to get Maxwell-Boltzmann distribution, Bose-Einstein distribution and Fermi-Dirac distribution. Is the priciple of Maximum Entropy just a part of the priciple of least action?
+ And if we define the action<br>
  $$
  A = pV-TS.
  $$
  We can see the action is the sum of many conjugate variables, so the temperature $T$ and entropy $S$ are conjugate variables. We know the position $x$ and velocity $v$ are conjugate variables to describe a particle or some microscopic system, but it's impossible to use $x_i$ and $v_i$ to describe the whole macroscopic system which aren't connected to each other regularly. How do we know use $T$ and $S$? Beacause the temperature can change the system's energy?

+ We know ergodic hypothesis is implicit in these theory. But I doubt whether is it convinced to accept ergodic hypothesis. Beacause the states we can reach depend on the state we are on now. And from the knowledge I learn from protein folding, I think it's impossible from a huge protein to search all the state space $\Omega$. So we maybe consider the problem in accessible state space $\Omega_s$. So what dynamical result we can get from accessible state space on the priciple of Maxmium Entropy? And maybe that just show the way of non-equilibrium statistical physics.


Now we come to a crossroad on the way of statistical physics. In the original plan, 
+ I want to show the core theory and main weapon we can get from statistical physics, 
+ and show how to use the weapon to deal with various problems with common properties in many areas and get same useful results, 
+ and then show the applications of these theory, weapon and results in many field from particle physics to cosmology, to AMO and condensed matter physics, to biophysics especially protein folding and even to statistical phenomenon in social, 
+ and last show frontier study by many problems of statistical physics in these field. From now on we only use half of a month to learn the core theory and get main weapon such as Maxwell-Boltzmann distribution, Bose-Einstein distribution and Fermi-Dirac distribution. We should speed more time on this and walk deeper into the core theory. 

So where should we go? Fluctuation-Dissipation theorem, linear response theory, phase transition or pattern formation? We should walk into non-equilibrium statistical physics, but it's really complicated and chaotic. Can we continue use the principle of Maximum Entropy or choose one aspect of non-equilibrium statistical physics to learn and then decide where to go? Answer: Learn the Fluctuation-Dissipation theorem and the discuss the relationship between the principle of Maximum Entropy and Fluctuation-Dissipation theorem.

## Linear Response Theory and Fluctuation Dissipation Theorem

We consider the system interacts with environment, not just exchange energy and particle with the environment but also complicated interaction. So now the probability $p_i$ on state $s_i$ will depend on time $t$ and need to calculate $p_i(t)$ or $\rho(t)$. Can we get $\rho(t)$ also from the principle of Maximum Entropy? It's strange that all the materials about non-equilibrium statistical physics give up the principle of Maximum Entropy, even entropy in practice, to deal with the system but turn to find dynamical description of the system.

Let's show the dynamical methods below first. The time evolution of the system is given by <br>
$$
\frac{d\rho(t)}{dt} = -\left\{H(t),\rho(t) \right\}.
$$
The assumption in linear response theory is that $H(t)$ contains a weak perturbation on $A$ and the system will response linearly on $B$ (also on $\Delta\rho(t)$). And the central variables in fluctuation dissipation theorem is response function $\chi_{AB}(t)$ and correlation function $c_{AB}(t)$.<br>

$$
\chi_{AB}(t)=\int d\Gamma e^{iL_0t}\left\{A,\rho_0\right\}B=-\langle\left\{A(t),B\right\}\rangle_{\rho_0}=\beta\langle\dot A(t),B\rangle_{\rho_0}\\
c_{AB}(t)=\langle A(t),B\rangle_{\rho_0}.
$$
We can see response function and correlation function are very similar and we can show this more explicitly in frequency domain:<br>
$$
\begin{aligned}
\tilde{c}_{AB}(z=\omega+i\epsilon)&=\int_0^{\infty}dtc_{AB}(t)e^{izt}=\int_0^{\infty}dt\langle A(t),B\rangle_{\rho_0}e^{i(\omega+i\epsilon)t}\\
\chi_{AB}(z=\omega+i\epsilon)&=\int_0^{\infty}dt\chi_{AB}(t)e^{izt}\\
&=\int_0^{\infty}dt\beta\langle\dot A(t),B\rangle_{\rho_0}e^{i(\omega+i\epsilon)t}\\
&=\beta\int_0^{\infty}dt\left(\frac{d}{dt}\langle A(t),B\rangle_{\rho_0}\right)e^{i(\omega+i\epsilon)t}\\
&=\beta\langle A(t),B\rangle_{\rho_0}e^{i(\omega+i\epsilon)t}\big|_{t=0}^{\infty}-\beta\int_0^{\infty}dt\langle A(t),B\rangle_{\rho_0}e^{i(\omega+i\epsilon)t}i(\omega+i\epsilon)\\
&=\beta\langle A(\infty),B\rangle_{\rho_0}e^{i\omega\infty}e^{-\epsilon\infty}-\beta\langle A(0),B\rangle_{\rho_0}-\beta(i\omega-\epsilon)\tilde{c}_{AB}(z=\omega+i\epsilon)\\
\text{let }\epsilon\rightarrow0^+,&\text{ and we can get}\\
&\Im\chi_{AB}(\omega)=\omega\beta\pi c_{AB}(\omega).
\end{aligned}
$$
This fluctuation-dissipation theorem looks like just a beautiful theory but useless. Can we insight into the system and get untrivial information from fluctuation-dissipation theorem? We also show the quantum fluctuation-dissipation theorem:<br>
$$
S(\omega)=2\hbar\left[1+n(\hbar\omega)\right]\Im\chi(\omega).
$$


# Problems with common properties

## Combinational Entropy and Parameter Control
We now look into entropy and then we can see all kinds of concrete forms of entropy. In the simplest case, we use thermal entropy<br>
$$
S=k_b\ln W.
$$
The key variables here is the number of microstates. What is microstate? We assume the system consist of $N$ variables $x_i$ and we have macroscopic quanties $X(x_i)$, then the number of microstates depend on $X$.

### One dimension random walk and Force-Extension Curve

We know quantum theory and relativity theory are two cornerstones in modern physics. As said by Roman Frigg, statistical physics has not yet found a generally accepted theoretical framework or a canonical formalism. So how can we find the way to the framework? Follow the important questions and solve these questions in various circumstances.

Ergodicity.

One person says there exists one ergodic hierarchy:
$$
\text{Bernoulli}\subset\text{Kolmogorov}\subset\text{Mixing}\subset\text{Ergodic}.
$$
Is this true or false? Pure Mathematical view. Something related: Liouville's theorem, Poincare's recurrence theorem.