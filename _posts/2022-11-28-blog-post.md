---
title: 'Statistical Physics and Applications'
date: 2022-11-28
permalink: /posts/2022/11/28/blog-post/
tags:
  - Statistical Physics
---

# Purpose

I write this note to help myself learn and understand statistical physics. Also for using English in academic.

# Maximum Entropy Principle

The begin of statistical physics is *Maximum Entropy Principle*. So what is *Maximum Entropy Principle*, or more basicly, what is *Entropy*?

The object $O$ in *Maximum Entropy Principle* can have many *states* $s_i$ and what we care about is the possibility on these *states* $p_i$. I write this opinion down

$$
\begin{align}
&O \sim s_1,s_2,\cdots,s_N \notag \\
&S = S(p_1,p_2,\cdots,p_N) \notag
\end{align}
$$

So *Maximum Entropy Principle* will give a constraint *F* on the *states* $p_i$. But what is the explicit form of the constraint *F* under specific conditions? We must consider the concrete meaning of *states* ${s_i}$ and the explicit form of *Entropy* $S$ on these *states*.


## Uniform Distribution and Shannon Entropy

If the object $O$ has $N$ *states* totally. Now we can check the probability distribution is uniform distribution by *Maximum Entropy Principle* with Shannon Entropy $S = \sum_i-p_ilnp_i$. We use Lagrange Multiplier
$$
S = -\sum_i p_ilnp_i - \alpha(\sum_i p_i-1)
$$
When the probability $p_i$ has maximum entropy, we have
$$
\frac{\partial S}{\partial p_i} = -(lnp_i+p_i*\frac{1}{p_i}) - \alpha=0\\
lnp_i = -\alpha-1\\
p_i = e^{-\alpha-1}\\
\sum_i p_i = \sum_i e^{-\alpha-1} = N*e^{-\alpha-1} = 1\\
p_i = 1/N.
$$

Similarly, we can consider many other constraint. If every *state* $s_i$ has an energy $E_i$ and the average energy is constrained to a fixed value $E$, then
$$
S = -\sum_i p_ilnp_i - \alpha(\sum_i p_i-1) - \beta(\sum_i p_iE_i - E)\\
\frac{\partial S}{\partial p_i} = -(lnp_i+p_i*\frac{1}{p_i}) - \alpha-\beta E_i=0\\
lnp_i = -\alpha-\beta E_i-1\\
p_i = e^{-\alpha-\beta E_i-1}\\
\sum_i p_i-1 = \sum_i e^{-\alpha-\beta E_i-1} - 1 = 0\\
e^{-\alpha-1} = \frac{1}{\sum_i e^{-\beta E_i}} \\
p_i = \frac{e^{-\beta E_i}}{\sum_i e^{-\beta E_i}}.
$$
This is just Maxwell-Boltzmann distribution.

### Thermal Entropy

We know the thermal entropy $S=k_blnW$, where $W$ is the number of microstates. Now we show that we can get thermal entropy from Shannon entropy(Gibbs entropy strictly).

$$
S = -\sum_{i=1}^{W} p_ilnp_i\\
S = -\sum_{i=1}^{W} \frac{1}{W}ln\frac{1}{W}\\
S = lnW
$$

### Von-Neumann Entropy

When we walk into quantum mechanics, we should consider von-Neumann entropy $S = -{\rm Tr}(\rho ln\rho)$. Let we show von-Neumann entropy is an extension of Shannon entropy(Gibbs entropy)
$$
S = -{\rm Tr}(\rho ln\rho)\\
if \rho = \sum_i p_i|i\rangle\langle i|\\
S = -{\rm Tr}[(\sum_i p_i|i\rangle\langle i|)ln(\sum_i p_i|i\rangle\langle i|)]\\
S = -{\rm Tr}[(\sum_i p_i|i\rangle\langle i|)(\sum_i lnp_i|i\rangle\langle i|)]\\
S = -{\rm Tr}(\sum_i p_ilnp_i|i\rangle\langle i|)=-\sum_i p_ilnp_i
$$
Now let we see what we can get from *Maximum Entropy Principle* with von-Neumann entropy. The answer is Bose-Einstein distribution and Fermi-Dirac distribution. We must consider what is the meaning of maximum entropy for $\rho$ and what is the constrains in quantum mechanics?

+ We firstly define the system and constrains: The energy and the number of particles in the system is conserved.
$$
{\rm Tr}(\rho\hat{H})=E\\
{\rm Tr}(\rho\hat{N})=N\\
\left[\hat{H},\hat{N}\right]=0
$$
Beacause Hamiltonian and the particle number operator commute, there exists a basis in which both are diagonal. We choose the basis $|i\rangle$ to consider the problem in hand. Remember now Hamiltonian and the particle number operator are diagonal, but the density matrix $\rho$ is uncertain.

+ If we use Langrange multiplier as in classical mechanics, we can get

$$
S = -{\rm Tr}(\rho ln\rho) - \alpha({\rm Tr}(\rho\hat{H})-E) - \beta({\rm Tr}(\rho\hat{N})=N)\\
\hat{H},\hat{N}\text{ is diagonal, }S = -{\rm Tr}(\rho ln\rho) - \alpha(\sum_i\rho_{ii}H_{ii}-E) - \beta(\sum_i\rho_{ii}N_{ii}-N)
$$
so what is the partial derivative we should do now?
$$\frac{\partial S}{\partial\rho_{ij}}=0$$

+ "Among all density matrices with the same diagonal (in some basis), the matrix with all entries outside the diagonal equal to zeros has the largest von-Neumann entropy".
This is an important consequence of Schur's theorem, but what is Schur's theorem?

  + If $A$ is a $n*n$ complex square matrix, then there is an unitary matrix $Q$ and a $n*n$ upper triangular matrix $U$:
  $$
  A=QUQ^H
  $$
  + von-Neumann entropy is a Schur-concave function and preserves the majorization order on density matrices. So are these two equivalent?

    + $$if \rho_1 \succ \rho_2, S(\rho_1)\geq S(\rho_2).$$
    + $$\sum_i\lambda_iS(\rho_i)-\sum_i\lambda_i\ln\lambda_i\geq S(\sum_i\lambda_i\rho_i)\geq\sum_i\lambda_iS(\rho_i)$$

+ Now we can only need to consider off-diagonal elements in $\rho$.
$$
S = -\sum_i\rho_{ii}\ln\rho_{ii} - \alpha(\sum_i\rho_{ii}H_{ii}-E) - \beta(\sum_i\rho_{ii}N_{ii}-N)
$$
Now the problem is similar as in classical mechanics, we can get
$$
\rho = \frac{e^{-\beta(\hat{H}-\mu\hat{N})}}{\rm Tr(e^{-\beta(\hat{H}-\mu\hat{N})})}
$$
In order to get Bose-Einstein distribution and Fermi-Dirac distribution, we must assume the particles are non-interacting. That's meaning
$$
\hat{H}=\sum_iE_i\hat{N}_i
$$
so we can calculate the partition function
$$
\begin{aligned}
Z &= \rm Tr(e^{-\beta(\hat{H}-\mu\hat{N})}) \\
&= \sum_{N=0}^{\infty}\sum_{N_i,\sum N_i=N}\prod_{i}e^{-\beta(E_iN_i-\mu N_i)} \\
&= \sum_{N=0}^{\infty}\sum_{N_i,\sum N_i=N}\prod_{i}e^{-\beta N_i(E_i-\mu)} \\
&= \prod_{i}\sum_{N=0}^{\infty}\sum_{N_i,\sum N_i=N}e^{-\beta N_i(E_i-\mu)}
\end{aligned}
$$
now we can see the difference between boson and fermion
$$
\begin{aligned}
&\text{for boson}, \sum_{N=0}^{\infty}\sum_{N_i,\sum N_i=N}e^{-\beta N_i(E_i-\mu)}=\sum_{N_i=0}^{\infty}e^{-\beta N_i(E_i-\mu)}=\frac{1}{1-e^{-\beta(E_i-\mu)}}\\
&\text{for fermion}, \sum_{N=0}^{\infty}\sum_{N_i,\sum N_i=N}e^{-\beta N_i(E_i-\mu)}=1+e^{-\beta(E_i-\mu)}
\end{aligned}
$$
then we can get the partition in a unity form
$$
Z = \prod_{i}(1+(-1)^se^{-\beta(E_i-\mu)})^{1-2s}.
$$
Then we use the partition function to calculate the number of particles in state $i$
$$
\begin{aligned}
\rm Tr[\rho N_i] &= -\frac{1}{\beta}\frac{\partial}{\partial E_i}\ln Z\\
&=-\frac{1}{\beta}\frac{\partial}{\partial E_i}\sum_i\ln(1+(-1)^se^{-\beta(E_i-\mu)})^{1-2s}\\
&=-\frac{1}{\beta}(1-2s)\frac{(-1)^se^{-\beta(E_i-\mu)}\cdot(-\beta)}{1+(-1)^se^{-\beta(E_i-\mu)}}\\
&=(1-2s)\frac{(-1)^se^{-\beta(E_i-\mu)}}{1+(-1)^se^{-\beta(E_i-\mu)}}.
\end{aligned}
$$
And this is just
$$
\begin{aligned}
&\text{for boson}, \rm Tr[\rho N_i] = -\frac{-e^{-\beta(E_i-\mu)}}{1-e^{-\beta(E_i-\mu)}}=\frac{1}{e^{\beta(E_i-\mu)}-1}\\
&\text{for fermion}, \rm Tr[\rho N_i] = \frac{e^{-\beta(E_i-\mu)}}{1+e^{-\beta(E_i-\mu)}}=\frac{1}{e^{\beta(E_i-\mu)}+1}.
\end{aligned}
$$

#### History

Firstly expounded by E. T. Jaynes(J in JC model).



#### Microstates and Macrostates

The states can be microstates and also macrostates. When we say they are microstates, that means we can't or needn't find any internal variables or freedoms to divide one microstate into more states. So every microstate has equal weight, that means the possibility of each microstate is equal to $1/N$, where $N$ is the total number of microstates. So we have microstates but why do we need macrostates? Beacause many microstates can have same features and too many microstates is meaningless expect their huge total number $N$. Nothing else we can know. And what we care about are a few important physical variables describing the objects. So can we fetch the information about these physical variables? The answer is just *Maximum Entropy*, or more accurately, following the direction and method shown by *Maximum Entropy*.


### Thinking and Discussion
+ Now we use Maximum Entropy to get Maxwell-Boltzmann distribution, Bose-Einstein distribution and Fermi-Dirac distribution. Is the priciple of Maximum Entropy just a part of the priciple of least action?
+ And if we define the action
  $$
  A = pV-TS.
  $$
  We can see the action is the sum of many conjugate variables, so the temperature $T$ and entropy $S$ are conjugate variables. We know the position $x$ and velocity $v$ are conjugate variables to describe a particle or some microscopic system, but it's impossible to use $x_i$ and $v_i$ to describe the whole macroscopic system which aren't connected to each other regularly. How do we know use $T$ and $S$? Beacause the temperature can change the system's energy?

+ We know ergodic hypothesis is implicit in these theory. But I doubt whether is it convinced to accept ergodic hypothesis. Beacause the states we can reach depend on the state we are on now. And from the knowledge I learn from protein folding, I think it's impossible from a huge protein to search all the state space $\Omega$. So we maybe consider the problem in accessible state space $\Omega_s$. So what dynamical result we can get from accessible state space on the priciple of Maxmium Entropy? And maybe that just show the way of non-equilibrium statistical physics.


Now we come to a crossroad on the way of statistical physics. In the original plan, 
+ I want to show the core theory and main weapon we can get from statistical physics, 
+ and show how to use the weapon to deal with various problems with common properties in many areas and get same useful results, 
+ and then show the applications of these theory, weapon and results in many field from particle physics to cosmology, to AMO and condensed matter physics, to biophysics especially protein folding and even to statistical phenomenon in social, 
+ and last show frontier study by many problems of statistical physics in these field. From now on we only use half of a month to learn the core theory and get main weapon such as Maxwell-Boltzmann distribution, Bose-Einstein distribution and Fermi-Dirac distribution. We should speed more time on this and walk deeper into the core theory. 

So where should we go? Fluctuation-Dissipation theorem, linear response theory, phase transition or pattern formation? We should walk into non-equilibrium statistical physics, but it's really complicated and chaotic. Can we continue use the principle of Maximum Entropy or choose one aspect of non-equilibrium statistical physics to learn and then decide where to go? Answer: Learn the Fluctuation-Dissipation theorem and the discuss the relationship between the principle of Maximum Entropy and Fluctuation-Dissipation theorem.

## Maxwell-Boltzmann Distribution

We consider the model *M* consist of the system *S* and a thermal bath *B*. The total energy $E_t$ and particle numbers $N_t$ in *M* are conserved. But the system *S* and bath *B* can exchange energy and particle with each other.

### Only exchange energy

Let me define the conditions more accurately with mathmatics

$$E_t = E_s + E_b = Const$$

$$W_t = W_s*W_b$$

(Assumption of independence between *S* and *B*, but if they have correlation?)
Now we want to maximize $W_t$ and need to point out the relation between the number of microscopic states $W$ and energy $E$.

